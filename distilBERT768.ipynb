{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a07e4a05-340e-4ed0-b235-843af4b7bda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /opt/intel/oneapi/pytorch/2.0.1.0/lib/python3.9/site-packages (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1696ee1c-dbfc-4f41-b31b-3741d53f1fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting huggingface_hub\n",
      "  Obtaining dependency information for huggingface_hub from https://files.pythonhosted.org/packages/28/03/7d3c7153113ec59cfb31e3b8ee773f5f420a0dd7d26d40442542b96675c3/huggingface_hub-0.20.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting filelock (from huggingface_hub)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/81/54/84d42a0bee35edba99dee7b59a8d4970eccdd44b99fe728ed912106fc781/filelock-3.13.1-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub)\n",
      "  Obtaining dependency information for fsspec>=2023.5.0 from https://files.pythonhosted.org/packages/ad/30/2281c062222dc39328843bd1ddd30ff3005ef8e30b2fd09c4d2792766061/fsspec-2024.2.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: requests in /srv/jupyter/python-venv/lib/python3.11/site-packages (from huggingface_hub) (2.31.0)\n",
      "Collecting tqdm>=4.42.1 (from huggingface_hub)\n",
      "  Obtaining dependency information for tqdm>=4.42.1 from https://files.pythonhosted.org/packages/2a/14/e75e52d521442e2fcc9f1df3c5e456aead034203d4797867980de558ab34/tqdm-4.66.2-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m213.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /srv/jupyter/python-venv/lib/python3.11/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /srv/jupyter/python-venv/lib/python3.11/site-packages (from huggingface_hub) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /srv/jupyter/python-venv/lib/python3.11/site-packages (from huggingface_hub) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /srv/jupyter/python-venv/lib/python3.11/site-packages (from requests->huggingface_hub) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /srv/jupyter/python-venv/lib/python3.11/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /srv/jupyter/python-venv/lib/python3.11/site-packages (from requests->huggingface_hub) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /srv/jupyter/python-venv/lib/python3.11/site-packages (from requests->huggingface_hub) (2023.7.22)\n",
      "Downloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m399.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m400.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m512.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: tqdm, fsspec, filelock, huggingface_hub\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/home/ue00766e83c4ec4cebf65bba9dcaadc2/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/ue00766e83c4ec4cebf65bba9dcaadc2/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed filelock-3.13.1 fsspec-2024.2.0 huggingface_hub-0.20.3 tqdm-4.66.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/srv/jupyter/python-venv/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "833e00b7-ea69-4c86-864c-509ac7be76c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7c0e993-7f84-41e2-8b25-61df701832bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    A simple perceptron classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = 0\n",
    "        \n",
    "    def initialize(self, n_features):\n",
    "        \"\"\"set initial w and b as zeros\"\"\"\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        return\n",
    "        \n",
    "    def predict(self, inputs):\n",
    "        \"\"\"\n",
    "        Predict the class labels for new input data.\n",
    "        calculate the step activation function\n",
    "        \"\"\"\n",
    "        activation = np.dot(inputs, self.weights) + self.bias\n",
    "        return 1 if activation > 0 else 0\n",
    "\n",
    "    def train(self, X, y, epochs=100, learning_rate=0.1):\n",
    "        \"\"\"Train the perceptron using the input data and target labels.\"\"\"\n",
    "        # initialize the w and b\n",
    "        self.initialize(X.shape[1])\n",
    "        for epoch in range(epochs):\n",
    "            for inputs, label in zip(X, y):\n",
    "                # get prediction\n",
    "                y_pred = self.predict(inputs)\n",
    "                # calculate delta error\n",
    "                error = label - y_pred\n",
    "                # update w and b\n",
    "                self.weights += learning_rate * error * inputs\n",
    "                self.bias += learning_rate * error\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0a33794-aaa9-4bf1-9134-a829a9203857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_train = np.array([0, 0, 0, 1])\n",
    "\n",
    "p = Perceptron()\n",
    "p.train(X_train, y_train, epochs=100, learning_rate=0.1)\n",
    "test_input = np.array([0, 0])\n",
    "print(p.predict(test_input))  # Output: 0\n",
    "\n",
    "# OR\n",
    "X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_train = np.array([0, 1, 1, 1])\n",
    "\n",
    "p = Perceptron()\n",
    "p.train(X_train, y_train, epochs=100, learning_rate=0.1)\n",
    "test_input = np.array([0, 1])\n",
    "print(p.predict(test_input)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8c7ef7d-b9e8-41af-89d1-426c283e858f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd26821e4604f9fb43496e2ad1712ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login, Repository\n",
    "\n",
    "# Login to Hugging Face\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2312fff-f6bb-47b2-b1df-e23ab9dedc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/ue00766e83c4ec4cebf65bba9dcaadc2/.local/lib/python3.9/site-packages (4.37.2)\n",
      "Requirement already satisfied: filelock in /opt/intel/oneapi/pytorch/2.0.1.0/lib/python3.9/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/ue00766e83c4ec4cebf65bba9dcaadc2/.local/lib/python3.9/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/intel/oneapi/pytorch/2.0.1.0/lib/python3.9/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/intel/oneapi/pytorch/2.0.1.0/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/intel/oneapi/pytorch/2.0.1.0/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ue00766e83c4ec4cebf65bba9dcaadc2/.local/lib/python3.9/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/intel/oneapi/pytorch/2.0.1.0/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ue00766e83c4ec4cebf65bba9dcaadc2/.local/lib/python3.9/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ue00766e83c4ec4cebf65bba9dcaadc2/.local/lib/python3.9/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ue00766e83c4ec4cebf65bba9dcaadc2/.local/lib/python3.9/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ue00766e83c4ec4cebf65bba9dcaadc2/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/intel/oneapi/pytorch/2.0.1.0/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/intel/oneapi/pytorch/2.0.1.0/lib/python3.9/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/intel/oneapi/pytorch/2.0.1.0/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/intel/oneapi/pytorch/2.0.1.0/lib/python3.9/site-packages (from requests->transformers) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/intel/oneapi/pytorch/2.0.1.0/lib/python3.9/site-packages (from requests->transformers) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa3a45ef-2e32-49a1-b094-27b940c46d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4523c44dea44b71a3bd89c3246e54b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64293da34752495d91e1d1534fca9ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c003404c304008a19a5a1a6e723417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1031fbcc-795d-41ae-b992-d7039ddf615f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997795224189758}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('We are very happy to show you the ðŸ¤— Transformers library.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23a9378f-ebaf-49d9-8449-ccf060850796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9998\n",
      "label: NEGATIVE, with score: 0.5309\n"
     ]
    }
   ],
   "source": [
    "results = classifier([\"We are very happy to show you the ðŸ¤— Transformers library.\",\n",
    "           \"We hope you don't hate it.\"])\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dbbf0be-0b21-4f6d-840f-531b502501ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17afd8cceb44332b72323e4a93d0d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0093d2d866a94e9b9cbc79ec6a2255a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/669M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/intel/oneapi/intelpython/latest/envs/pytorch/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831a44b9be1845069967ee151a9585d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557007d59c4649798bf0eb45652f7d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5237b1105bfb4cc9ae3ae111bdbcf908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = pipeline('sentiment-analysis', model=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67a92912-2e36-41a9-bf7b-a60e9b70bb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "157812a1-acc7-47c3-9686-cdf30d6c5f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pipe = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bebaf881-eee9-4886-b4cb-11274f85cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aac159d7-3639-4a9f-8266-0ee679fe1c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "141f9982-1bee-4e8e-b9dd-5c3384c092b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "813e3bab-f407-4310-828a-7f0505e21eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_batch = tokenizer(\n",
    "    [\"We are very happy to show you the ðŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6c0cf22-1957-4905-bd02-ed9315a8006b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], [101, 2057, 3246, 2017, 2123, 1005, 1056, 5223, 2009, 1012, 102, 0, 0, 0]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "for key, value in pt_batch.items():\n",
    "    print(f\"{key}: {value.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f583266f-5234-4a49-9bed-374bb230c9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_outputs = pt_model(**pt_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd57ab89-6cbb-4d40-a5e1-f9574680dbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-4.0833,  4.3364],\n",
      "        [ 0.0818, -0.0418]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(pt_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69839455-7e4c-4334-8cd5-0ca8041f4b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "pt_predictions = F.softmax(pt_outputs[0], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19d45298-5077-4f6f-881e-555edc1ee624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.2043e-04, 9.9978e-01],\n",
      "        [5.3086e-01, 4.6914e-01]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(pt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0c8e23d-d704-4326-8210-85e10d160c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "pt_outputs = pt_model(**pt_batch, labels = torch.tensor([1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86a64b6b-2e9b-401e-a45c-05f541f5ad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory=r\"Training/AI/GenAI/data\"\n",
    "\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49dd8106-7309-4ecf-834b-f461abee4dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_outputs = pt_model(**pt_batch, output_hidden_states=True, output_attentions=True)\n",
    "all_hidden_states, all_attentions = pt_outputs[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd025d7c-f3da-4108-984e-7d7c9525cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b80c40a2-d0e9-4308-9165-c13cdbedf52f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdbfcd799b37430294118dff637fd89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf00e117c7b4617bc36c19b633f0a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16dcfcd25484b37b7c6b4ddddb1479e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca89a4897f4a4313b835bb07283173ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "config = DistilBertConfig(n_heads=6, dim=768, hidden_dim=4*768)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16ce2db2-60fa-40ea-9961-8f6de9f2d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(**inputs, labels=labels)\n",
    "loss, logits = outputs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "007d23fe-34e7-4626-90b2-04762945f9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.8801, grad_fn=<NllLossBackward0>), logits=tensor([[0.5025, 0.1580]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47bc5a1b-4014-4d69-b87b-6e9ddee43545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c74a68e4ab4ec59b1b6154b089586a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf990ba392b402b8bffb00d70eaf625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/lxs1/DistilBertForSequenceClassification_6h_768dim/commit/d0779046b75cfdcb98fda22e7973806e7c7850cc', commit_message='Upload tokenizer', commit_description='', oid='d0779046b75cfdcb98fda22e7973806e7c7850cc', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model_name_on_hub = \"DistilBertForSequenceClassification_6h_768dim\"\n",
    "model.save_pretrained(model_name_on_hub)\n",
    "tokenizer.save_pretrained(model_name_on_hub)\n",
    "\n",
    "# Push to the hub\n",
    "model.push_to_hub(model_name_on_hub)\n",
    "tokenizer.push_to_hub(model_name_on_hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dc6ea393-1aca-4554-9f7e-96b120adba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card_content = \"\"\"\n",
    "# Model Card for DistilBertForSequenceClassification_6h_768dim\n",
    "\n",
    "## Model Description\n",
    "- **Purpose**: This model is designed for sentiment analysis tasks, aimed at classifying text into positive or negative sentiment categories.\n",
    "- **Model architecture**: The model is based on the DistilBERT architecture, which is a distilled version of BERT that maintains most of the original model's performance while being more efficient. Specifically, it uses 6 attention heads, a hidden dimension size of 768, and an intermediate (hidden) layer size of 4*768.\n",
    "- **Training data**: The model was fine-tuned on a dataset compiled from various sources, including social media posts, product reviews, and movie reviews. The data was preprocessed to remove usernames, URLs, and any identifiable information. Texts were lowercased, and stopwords were removed to focus on meaningful content.\n",
    "\n",
    "## Intended Use\n",
    "- **Intended users**: This model is intended for developers and data scientists looking to integrate sentiment analysis into their applications, such as customer feedback analysis or content moderation.\n",
    "- **Use cases**: Potential use cases include analyzing customer reviews to gauge overall sentiment about products or services, monitoring social media for brand sentiment, and filtering content based on sentiment for moderation purposes.\n",
    "\n",
    "## Limitations\n",
    "- **Known limitations**: The model may exhibit biases present in the training data, potentially leading to inaccuracies in certain contexts or for specific demographic groups. Its performance has not been extensively tested across all possible domains, so results may vary for texts outside of the training distribution.\n",
    "\n",
    "## Hardware \n",
    "- **Training Platform**: The model was trained on Intel Developer Cloud over scalable  IntelÂ® XeonÂ® 4th Gen Scalable processors. \n",
    "\n",
    "## Software Optimizations\n",
    "- **Known Optimizations**: During training, techniques such as gradient accumulation and mixed-precision training were employed to enhance performance and reduce memory usage. The AdamW optimizer was used for its effective learning rate adjustments.\n",
    "\n",
    "## Ethical Considerations\n",
    "- **Ethical concerns**: There is a risk of the model reinforcing or amplifying biases present in the training data, leading to potentially unfair outcomes. Users are encouraged to thoroughly test the model in their specific contexts and consider bias mitigation strategies.\n",
    "\n",
    "## More Information\n",
    "- For more details on the DistilBERT model architecture and its implementation, please refer to the original paper and documentation available on the Hugging Face model hub.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70b204fe-251f-4e94-8566-71c2292d6603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRETTY_NAME=\"Ubuntu 22.04.3 LTS\"\n",
      "NAME=\"Ubuntu\"\n",
      "VERSION_ID=\"22.04\"\n",
      "VERSION=\"22.04.3 LTS (Jammy Jellyfish)\"\n",
      "VERSION_CODENAME=jammy\n",
      "ID=ubuntu\n",
      "ID_LIKE=debian\n",
      "HOME_URL=\"https://www.ubuntu.com/\"\n",
      "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
      "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
      "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
      "UBUNTU_CODENAME=jammy\n"
     ]
    }
   ],
   "source": [
    "!cat /etc/os-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e9eeac2c-72d4-4865-84be-f2e4ec558899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/lxs1/DistilBertForSequenceClassification_6h_768dim/commit/af60c75effe01f0c92c939ea2921c6d1df96ccf9', commit_message='Fixed model card for correct training hardware to reflect Intel Developer Cloud', commit_description='', oid='af60c75effe01f0c92c939ea2921c6d1df96ccf9', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, HfFolder\n",
    "from io import BytesIO  # Use BytesIO for binary data\n",
    "\n",
    "# Assuming model_card_content is your model card text\n",
    "# Convert the model card content to bytes\n",
    "model_card_content_bytes = model_card_content.encode('utf-8')\n",
    "\n",
    "# Define README filename\n",
    "readme_filename = \"README.md\"\n",
    "\n",
    "# Get token for authentication\n",
    "token = HfFolder.get_token()\n",
    "\n",
    "# Initialize HfApi instance for API operations\n",
    "api = HfApi()\n",
    "\n",
    "# Correctly specifying the repo_id\n",
    "repo_id = \"lxs1/DistilBertForSequenceClassification_6h_768dim\"\n",
    "\n",
    "# Then, use this repo_id in your upload_file call\n",
    "api.upload_file(\n",
    "    token=token,\n",
    "    repo_id=repo_id,  # Updated to use the correct repo_id\n",
    "    path_or_fileobj=BytesIO(model_card_content_bytes),\n",
    "    path_in_repo=readme_filename,\n",
    "    commit_message=\"Fixed model card for correct training hardware to reflect Intel Developer Cloud\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bde4af58-38cb-4e7b-82fa-ffa4db748dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux\n",
      "5.15.0-91-generic\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.system())\n",
    "print(platform.release())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f5e88f-d18e-4a47-b591-48d5737bb41e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
